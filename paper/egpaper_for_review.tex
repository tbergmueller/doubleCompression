\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{icb}
\usepackage{times}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{amssymb}

\DeclareMathOperator*{\argmin}{arg\,min}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%\icbfinalcopy % *** Uncomment this line for the final submission

\def\icbPaperID{****} % *** Enter the IJCB Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificbfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Recompression effects in iris segmentation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}






\maketitle
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous ICB abstracts to get a feel for style and length. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Iris recognition \cite{BBurge13a,Rathgeb12e} is one of the most deployed biometric
modalities, standardized by the International Civil Aviation
Organization (ICAO) for use in future passports, and one of
the technologies in the Unique Identification Authority of
India's (UID) Aadhaar project to uniquely identify Indian
citicens. However, the increasing market saturation of
biometric instead of conventional access control methods
raises the need for efficient means to store such data. The
International Organization for Standardization (ISO)
specifies iris biometric data to be recorded and stored in (raw)
image form (ISO/IEC FDIS 19794-6), rather than in extracted
templates (e.g. iris-codes). Such
deployments benefit from future improvements (e.g. in
feature extraction stage) which can be easily incorporated
without re-enrollment of registered users. Since biometric templates may depend on patent-registered
algorithms, databases of raw images also enable more
interoperability and vendor neutrality \cite{Rathgeb12e}. These facts
motivate detailed investigations and optimisations of image
compression on iris biometrics in order to provide an efficient
storage and rapid transmission of raw biometric records.
Furthermore, the application of low-powered mobile sensors
for image acquisition, e.g. mobile phones, raises the need for
reducing the amount of transmitted data.

The certainly most relevant standard for compressing
image data relevant in biometric systems is the ISO/IEC
19794 standard on Biometric Data Interchange Formats,
where in the most recently published version (ISO/IEC FDIS
19794-6), only JPEG2000 is included for lossy compression.
JPEG2000 has also been recommended for various
application scenarios and standardised iris images (IREX
records) by the NIST Iris Exchange\footnote{IREX
\url{http://iris.nist.gov/irex/}} program. The ANSI/NIST-ITL
1-2011 standard on ``Data Format for the Interchange of
Fingerprint, Facial \& Other Biometric Information'' (former
ANSI/NIST-ITL 1-2007) also supports only JPEG2000 for
applications tolerating lossy compression.

As a consequence, according to the importance of this issue, many studies comparing and optimising lossy compression techniques for iris imagery 
may be found in the literature. Since the CASIA iris datasets have been very popular among researchers ever since their establishment,
many papers dealing with compression have been relying on the (extended) CASIA V1.0 dataset, including also first IREX investigations 
\cite{BRakshit07a,BIves08a,Matschitsch07a,Haemmerle09a,Konrad09a} (apart from other examples using the ICE 2005 dataset 
% noch V1: BIves05a,Konrad09b,Kostmajer09a,BRakshit06a,
\cite{BDaugman08a,BIves10a}).
	
However, since it has been pointed out \cite{BPhilips07a} that the CASIA V1.0 dataset exhibits manipulated pupil areas and should therefore not be used any further in experimentation, compression researchers moved to other (and more recent, more challenging etc.) datasets, e.g. 
	the CASIA V3.0 \cite{Horvath11b,Rathgeb12e}, 
	the CASIA V4.0 \cite{BTuba12a}, the Bath \cite{BIves08a,BPardamean12a}, and the UBIRIS.v1 \cite{Haemmerle09a,BCarneiro11a} datasets.
	While the images of CASIA V1.0 and ICE 2005 are given in uncompressed format, images in CASIA V3.0, CASIA V4.0, UBIRIS and Bath datasets are provided as JPEG (the first three) or JPEG2000 (the latter) lossy compressed data. Therefore, any compression experiments conducted on these datasets operate on pre-compressed data. 
	
	This fact has not been ignored entirely -- for example, in \cite{Rathgeb12e}, preparatory JPEG compression experiments with uncompressed data reveal that 
	slightly pre-compressed data leads to better recognition performance due to denoising effects. Thus experiments with pre-compressed data are assessed to
	be unproblematic. The same argument is used for JPEG2000 pre-compressed data \cite{BPardamean12a} based on the results in \cite{BIves08a}.
	However, eventual artifacts resulting from recompression effects are not accounted for in these considerations. Recompression artifacts arise in cases where	data is compressed twice (or multiple times) with lossy compression schemes, i.e. where artifacts from the first compression step (termed pre-compression)
	are aggravated or exploited by the second compression step.
	
	Two different types of such effects may be distinguished: First, \emph{intra-recompression}, where the same compression scheme is used several times, whereas
	in \emph{inter-recompression} different methods are used in the different compression steps. For example, using JPEG pre-compressed data and applying
	JPEG XR and JPEG2000 \cite{Horvath11b} or JPEG2000 and fractal compression \cite{BCarneiro11a} is eventually prone to inter-recompression
	artifacts, while the application of JPEG to JPEG pre-compressed data \cite{BTuba12a,Rathgeb12e} can be prone to intra-recompression artifacts.
	
	While next to nothing can be found on the issue of inter-recompression artifacts in the general compression literature, 
	intra-recompression artifacts are better investigated, at least in the case of lossy JPEG compression. Soon after the establishment of
	the JPEG standard \cite{Pennebaker93a}, it was found that JPEG recompression artifacts arise and do not follow a linear behaviour \cite{Chan92a}.
	Extensive experiments in this direction can also  be found in \cite{Kumar11a}, and following these obversations, requantisation-based schemes have been
	suggested for JPEG, reducing recompression artifacts considerably \cite{Bauschke03a}. Recently, the identification of images which underwent 
	JPEG double compression (i.e. JPEG intra-recompression) has been a hot topic in image forensics \cite{Sencar12a}.
	
	Taking all these facts together, it gets clear that recompression artifacts may impact experimental results with respect to biometric recognition
	performance, an issue, that has been neglected so far. \cite{severeCompression, BDaugman08a} conclude that data reduction, i.e. compression, has the highest impact on the segmentation step of an iris recognition system because of segmentation errors.
	
	In this paper, we systematically investigate eventual intra- and inter-recompression effects 
	in an experimental study for iris segmentation. Given the importance of JPEG in the area (as the CASIA V3.0/V4.0 and UBIRIS.v1 datasets are only available in this format), we 
	focus on JPEG pre-compressed data. In our experiments, we compare iris recognition, iris segmentation, and general purpose image quality metrics applied
	to single compressed vs. recompressed (i.e. JPEG pre-compressed) iris image data. In Section \ref{section:comprScheme}, we discuss the aspects relevant when generating single and recompressed data. Under these aspects, data for experimental evaluation is generated as described in section \ref{section:exSetup}. Section \ref{section:eval} lists several experiments and their results, which are compared in section \ref{section:results}. From the experiments' individual and comparison results, we withdraw a conclusion in section \ref{section:conclusion}.
	

\section{Compression scheme}
\label{section:comprScheme}
As discussed, we investigate whether it makes a difference if truly uncompressed data or pre-compressed data is used in experiments rating the performance of an iris segmentation system. 
Using pre-compressed data in such experiments means a pre-compressed image $I_p$ is compressed a second time resulting in an image $I_r$. As discussed, we denote this process as \emph{recompression}. When compressing a truly uncompressed image $I_u$, which was read directly from the sensor, we denote this as \emph{single-compression} with a resulting image $I_s$. Since experiments are typically carried out on a data set with more than one image, we denote $I_u^{(k)}, I_p^{(k)}, I_s^{(k)}, I_r^{(k)} \in \mathbb{R}^{w \times h}$ as the $k^{th}$ image with width $w$ and height $h$ in the particular data sets. Furthermore, we define $s(F) \in \mathbb{N}$ with $F \sim{I \in \mathbb{R}^{w \times h}}$ as a function that returns the file size of the file $F$ storing an image $I \in \mathbb{R}^{w \times h} $. Since common lossy compression algorithms also employ lossless compression methods, e.g. run-length encoding, before writing to a file, $F$ is loosely linked to the pixel data, namely the image, $I$. For simplicity, we denote $s(I)$ as the file size of the file $F$ encoding the pixel values of an image $I$. $c_{m}(I, q)$ with $q \in \mathbb{N}$ describes the process of compressing an image $I$ using a particular method $m$ parametrized with the quality parameter $p$. In terms of this paper we use the values $m \in \{jpg, jxr, j2k\}$, where 
\begin{itemize}
	\item $jpg$ corresponds to the well-known (ISO/IEC IS 10918-1) DCT-based image compression method \cite{jpg},
	\item $j2k$ corresponds to the wavelet-based image compression standard (ISO/IEC IS 15444-1), which can operate at higher compression ratios \cite{j2k} and
	\item $jxr$ corresponds to a compression standard based on Microsoft’s HD Photo, which is specified in (ISO/IEC IS 29199-2) \cite{jxr}.  
\end{itemize}

%TODO: Consider: Do I have to specify which methods we used (Matlab JxrEncApp) to come up with the images? As it is a standard, this shouldn't be necessary, because everything operates the same anyway?!

For rating an image $I$'s compression effectiveness, we define the compression ratio $cr$ between an uncompressed image $I_u$ and a compressed image $I_c$ as 
\begin{equation}
cr(I_u, I_c) = \frac{s(I_u)}{s(I_c)} \quad \text{with} \quad I_c \in \{I_r, I_s\}
\end{equation}

For the later described experiments, images are compressed to a target compression ratio $cr_t \in \mathbb{R}$. However, only the \emph{j2k} compression standard \cite{j2k} allows to specify a target compression ratio $cr_t$ directly via parameter $q$. Hence this is the only method where we can control the file size $s(I_c)$ directly. The other two compression methods take a quality parameter $q \in \mathbb{N}$ only, which controls the quality but not the file size of the output image $I_c$. Therefore it is not possible to set this parameter in a way to meet a certain target compression ratio $cr_t$. Due to the limited universe of the quality parameters, the target compression ratio $cr_t$ cannot be achieved exactly in any case. However, parameter optimization can be done, such that $cr_t \approxeq cr(I_u^{(k)}, I_c^{(k)})$. We propose an algorithm to compress a set of $K$ uncompressed images $I_u$ using a particular method $m$ to achieve a certain compression ratio $cr_t$ in a way that the compression ratio of each image is met as close as possible.


\begin{figure}[h]
	\begin{center}
		
		\includegraphics[width=0.7\linewidth]{img/comprScheme}
	\end{center}
	\caption{Basic compression principle to obtain two images achieving approximately the same target compression ratio $cr_t$ from an uncompressed image $I_u^{(k)}$ using a particular compression method $m$. One image, $I_s^{(k)}$, is compressed in a single step while the other, $I_r^{(k)}$, uses a pre-compression and a final compression step. Note the pre-compression step is always a $jpg$-compression, while the final compression step uses the same method $m$ as used in the single-compression.}
	\label{fig:comprScheme}
	
\end{figure}

This process, illustrated in Fig. \ref{fig:comprScheme}, employs
\begin{enumerate}
	\item Compute the single-compressed image $I_s^{(k)}$ with method $m$ such that $cr(I_u^{(k)}, I_s^{(k)}) \approx cr_t$. The optimal quality parameter $q_s^{(k)}$ is computed for each image separately by
	\begin{eqnarray}
	\small
	s_t^{(k)} = \frac{s(I_u^{(k)})}{cr_t} \\
	q_s^{(k)} = \underset{q \in \mathbb{N}}{argmin}|s(c_m(I_u^{(k)},q)) - s_t^{(k)}|,
	\end{eqnarray} where $s_t^{(k)}$ is the file size exactly meeting the target compression ratio $cr_t$. This is implemented by iteratively searching the quality parameter $q$ that results in the closest achievable compression ratio $cr(I_u, I_c)$. The single compressed images $I_s^{(k)}$ using method $m$ are computed with the optimal parameters $ q_s^{(k)}$ as
	\begin{equation}
	I_s^{(k)} = c_m(I_u^{(k)}, q_s^{(k)})
	\end{equation}
	
	\item Compute a pre-compressed image $I_p^{(k)}$ representing images in pre-compressed data set using $jpg$-method \cite{jpg} with an arbitrary but fixed quality parameter $q_p$, i.e. 
	\begin{equation}
		I_p^{(k)} = c_{jpg}(I_u^{(k)}, q_p)
	\end{equation}
	
	\item Now, find a quality parameter $q_d^{(k)}$ that allows to compress the pre-compressed image $I_p^{(k)}$ a second time, such that the resulting recompressed image $I_r^{(k)}$ has the same file size as the single-compressed image $I_s^{(k)}$, i.e. $s(I_s^{(k)}) \approxeq s(I_r^{(k)})$. Such a quality parameter $q_r^{(k)}$ can be found by optimizing
	\begin{equation}
	\small
	q_r^{(k)} = \underset{q \in \mathbb{N}}{argmin}|s(c_m(I_p^{(k)},q)) - s(I_s^{(k)})| \quad \forall s(I_s^{(k)}) \geq s(I_r^{(k)})
	\label{equ:recomp}
	\end{equation}
	
	The condition $s(I_s^{(k)}) \geq s(I_r^{(k)})$ is of importance to establish fair conditions, since it is very likely that the file sizes $s(I_s^{(k)}), s(I_r^{(k)})$  cannot be equalized due to the limited universe of the quality parameters of the used compression methods $m$. The recompressed images $I_r^{(k)}$ are then computed from the pre-compressed images $I_p^{(k)}$ with the found optimal parameters $q_r$ as
	\begin{equation}
	I_r^{(k)} = c_m(I_p^{(k)}, q_r^{(k)})
	\end{equation}
\end{enumerate}

Using data sets generated with the described method, we can investigate the impact of recompression artifacts, contained in single compressed images $I_r$, in comparison to images $I_s$  containing artifacts of a single compression only.

\section{Experimental setup}
\label{section:exSetup}
Although there are several iris data sets around, few are available in uncompressed format. We use the IITD Iris data base\footnote{IITD Iris Database version 1.0, www4.comp.polyu.edu.hk/\textasciitilde csajaykr/IITD/Database\_Iris.htm}. The main reason for this is the availability of a segmentation ground truth created by an expert, which was recently introduced by Rathgeb \etal in \cite{severeCompression}. The $k^{th}$ image of this segmentation ground truth data set is subsequently denoted as $SGT^{(k)}$. They also give segmentation error results compareable to those we define in equation (\ref{equ:mserabs}).
We want to point out that according to information by the IITD iris data base's authors, the images stored in a 3-channel uncompressed bitmap format\footnote{We want to point out that storing in 1-channel bitmaps would be more efficient, since the images were captured in near-infrared. However, we use the size information of the 3-channel bitmap in computing compression ratios} are already JPEG-compressed with 100\% quality by the sensor. Since they are stored as bitmaps, all images have an identical file size of $s(I_u)$=230,454 bytes. Despite these aspects, using the IITD was necessary due to the available ground truth, for reasons discussed in section \ref{section:ser}. Futhermore, the IITD -- contrary to others, e.g. the ND-IRIS-0405 iris image dataset \cite{Bowyer_thend-iris-0405}-- is captured under favourable conditions, which allows for lower segmentation errors. This is necessary to distinguish between noise and recompression-effects. 

We use the scheme introduced in section \ref{section:comprScheme} to compress obtain data sets with target compression ratios
\begin{equation}
	cr_t \in \{15,20,25,30,35,40,45,50,55,60,65,70,75\}.
\end{equation}
For each of these target compression ratios $cr_t$, the pre-compression step in recompression mode is carried out with quality parameters
\begin{equation}
	q_p \in \{100, 80, 75, 70\}
\end{equation} 
in order to simulate different levels of pre-compression. Each of these combinations is used to compress with the introduced $jpg$, $j2k$ and $jxr$ methods. We start at compression ratio $cr_t=15$, because even a pre-compression with $q_p=100$ achieves - depending on the image's content - already a compression ratio of $cr(I_u, I_p) \approx 10$. For obvious reasons, no smaller compression ratio $cr(I_u, I_r) < cr(I_u, I_p)$ can be achieved in recompression.  This results in a total of 195 data sets with 2240 images each, whose distribution is shown and discussed in fig. \ref{fig:dataDistribution}.

\begin{figure}
	\begin{small}
	
	\includegraphics[width=0.45\linewidth]{img/jpgData.eps}
	\includegraphics[width=0.45\linewidth]{img/jxrData.eps}
	
	\vspace{3mm}
	\begin{tabular}{c|c|c|c||c|c|c}
		& \multicolumn{3}{c||}{single: $1-\frac{cr(I_u, I_s)}{cr_t}$ } & \multicolumn{3}{c}{recomp.: $ 1-\frac{cr(I_u, I_r)}{cr_t}$} \\
		\hline
		\textbf{\%} & \emph{jpg} & \emph{j2k} & \emph{jxr} & \emph{jpg} & \emph{j2k} & \emph{jxr} \\
		\hline
		$ \mu $ & -3.21 & -3.48	&  -4.33 &  -6.80&   -7.04 &   -9.49 \\
		$ \sigma$ &2.74 &2.51 &  2.87 & 4.83  & 4.23  & 4.34 \\
	\end{tabular}
	
	
	\vspace{2mm}
	
	\end{small}
	
	\caption{Scatter plots of measured compression ratios $cr(I_u, I_s)$ over $cr(I_u, I_r)$ for methods \emph{jpg} (left) and \emph{jxr} (right). The graphs indicate that the $s(I_s^{(k)}) \geq s(I_r^{(k)})$ condition from equ. (\ref{equ:recomp}) is satisfied. While this is indeed true for \emph{j2k} and \emph{jxr}, we observe a violation in 0.13\% of the cases for \emph{jpg} at $cr_t \geq 70$. JPEG is already working at it's boundaries at such high compression ratios. The table below reveals, that in average the aimed $cr_t$ is met with 3.67\% accuracy for single-compressed images $I_s$, while the re-compressed ones $I_r$ only reach 7.8\%. This is due to the limited universe of quality parameters $q, q_p$.}
	
	
	\label{fig:dataDistribution}
\end{figure}

\section{Evaluation}
\label{section:eval}
 Besides assessing the image quality with fully-referenced metrics, we investigate the behaviour of segmentation error rate and the system's EER in respect to the compression ratio.


\subsection{Full-referenced quality metrics}
Todo Lefteris:
\begin{itemize}
 \item Which quality metrics were in the selection
 \item Which were chosen
 \item Why have you chosen these
 \item Give a very very brief introduction (rather referencing!) of what the quality metrics are about and the most characteristic features
 \item Results and findings of this evaluation
\end{itemize}

\subsection{Segmentation error rates}
\label{section:ser}
In iris recognition, the segmentation of an iris image is considered as one of the most critical parts \cite{BDaugman08a, severeCompression}. We investigate the differences of single- and recompression as well as the aspects of using an absolute reference, e.g. a ground truth, or a relative one, e.g. the segmentation of the uncompressed images $I_u$ when computing the error.

\begin{figure}
\begin{center}

  \includegraphics[width=0.3\linewidth]{img/segMasks/gt.png}
  \includegraphics[width=0.3\linewidth]{img/segMasks/jpg_caht_q100_cr5.png}
  \includegraphics[width=0.3\linewidth]{img/segMasks/jpg_wahet_q100_cr5.png}
  \end{center}
  
  \label{fig:segMasks}
  \caption{Segmentation masks of the expert ground truth \cite{severeCompression}, relative groundtruth $seg(I_u^{(k)})$ and an actual segmentation result $seg(I_r^{(k)})$ (f.l.t.r)}
\end{figure}

%TODO discuss, why the eye lashes are notmasked in the algos


The segmentation accuracy is rated by the mean segmentation error rate, which corresponds to the suggested E1 error rate in the Noisy Iris Challenge Evaluation - Part I (NICE.I), also used in \cite{severeCompression}. We define the segmentation error rate $ser$ of a single image $I$ as
\begin{equation}
ser(R,S) = \overline{R \oplus S} \in [0,1]\quad with \quad R,S \in \{0,1\}^{w \times h},
\end{equation} where $R$ is the binarized reference segmentation and $S$ the binarized segmentation result of the same image $I$. The mean value of the pixel-wise exclusive-or is the percentage of pixels different in the segmented image $S$ in respect to the reference $R$. Due to multiple images in a data base, the mean segmentation error $mser$ is computed from $K$ images. In this experiment, we compute error rates for the single- and recompressed images $I_c \in {I_s, I_r}$. We define the absolute mean segmentation error $mser_{abs}$ in respect to the ground truth $SGT$ from \cite{severeCompression} and the relative mean segmentation error $mser_{rel}$ in respect to the segmentation of the uncompressed images $I_u$. By denoting the segmentation process of an image $I$ as $seg(I) \in \{0,1\}^{w \times h}$ we have
\begin{eqnarray}
mser_{abs} = \frac{1}{K}\sum_{k=1}^{K}ser(SGT^{(k)},seg(I_c^{(k)})) \label{equ:mserabs} \\
mser_{rel} = \frac{1}{K}\sum_{k=1}^{K}ser(seg(I_u^{(k)}),seg(I_c^{(k)})) \label{equ:mserrel} \\
\end{eqnarray}

The absolute segmentation error rate, hence also the development in respect to compression ratio can be considered to be optimal because of the available ground truth. However, for most data bases no such ground truth is available. Therefore we evaluate if a corresponding conclusion can be withdrawn from the relative segmentation error $mser_{rel}$ as well. The benefit of such a relation is that the $mser_{rel}$ can be computed for any arbitrary data set easily. 

\begin{figure}[h]
	\begin{center}
		
	\includegraphics[width=0.49\linewidth]{img/mser/jpeg_wahet}
	\includegraphics[width=0.49\linewidth]{img/mser/abs_mser_wahet_jpeg}
	
\end{center}
	\caption{Top: Relative (left) and absolute (right) segmentation results with WAHET on $jpg$-compressed data. Note that the $mser_{abs}$ is generally higher than $mser_{rel}$, because the tested algorithms ignore eyelids, but they are considered in the expert ground truth \cite{severeCompression}.}
	\label{fig:segResults}
	
\end{figure}

The data set described in section \ref{section:exSetup} is used to test the two iris segmentation algorithms, Contrast-adjusted Hough Transform (CAHT) and Weighted Adaptive Hough and Ellipsopolar Transform (WAHET), from the USIT Framework v1.0.3\footnote{as available at \url{http://wavelab.at/sources/} \cite{rathgeb}}. From figure \ref{fig:segResults} we observe that

\begin{enumerate}
 \item there is no significant difference between segmentation errors of single- and recompressed data for small and medium compression ratios, i.e. $ cr_t \leq 50 $.  \label{noDiff}
 \item For large compression ratios, i.e. $cr_t > 50$ , segmentation errors tend to be lower for single-compressed data  $I_s$ compared to recompressed data $I_r$. Thus here it makes a difference whether pre-compressed or uncompressed data has been used. \label{yesDiff} 
 
 \item Comparing $mser_{rel}$ and $mser_{abs}$ indicates that in most cases both metrics show the same trends, especially for medium and large compression ratios, i.e. $cr_t > 30$. For the WAHET segmentation, however, the results obtained with $jxr$ and $j2k$ compressed data limit this observation to large compression ratios, since there is a difference to be observed for $cr_t \leq 30$ in fig. \ref{fig:segResults}. Hence, for low compression ratios a ground truth is needed to reliably rate a compression method's performace on iris segmentation. \label{gtForLow}
 
 \item Despite that, for larger compression ratios a strong correlation between $mser_{rel}$ and $mser_{abs}$ is observed in all experiments. Therefore, there is no need for an expert-generated ground truth in this range. However, we do not see a method to establish a numerical relation, e.g. a linear transformation, between $mser_{rel}$ and $mser_{abs}$. \label{gtNoHigh}
 
 \item While \emph{jxr} shows generally a linear growth in segmentation error rate, \emph{jpg} has exponential characteristics. This implies that segmentation algorithms, e.g. WAHET, might perform better with a certain extent of compression artifacts in the image. We expect this is due to the fact that compression artifacts are strongest near the edges, hence artifacts enhance the image to a certain extent for edge-detection based segmentation algorithms. \label{exponentialGrowth}
 
 
 \item Not surprisingly, \emph{jxr} outperforms \emph{jpg} in all settings and is therefore superior. \label{bestMethods}
\end{enumerate}

From \ref{noDiff} and \ref{yesDiff} we conclude, that for experiments studying severe image compression, e.g. \cite{BDaugman08a, severeCompression}, results cannot be considered entirely reliable when a pre-compressed data set has been used. Many studies resort back to precompressed data sets for the sake of ground truth availability. From \ref{gtNoHigh} we know that for large compression ratios, i.e. $cr_t > 50$, there is no difference in the progress of $mser_{rel}$ and $mser_{abs}$. Since this is (from \ref{yesDiff}) exactly the range, where using single- or recompressed data has an impact, we propose - based on observations \ref{gtForLow} and \ref{gtNoHigh} - to bench-mark compression algorithms in respect to iris segmentation by using 
\begin{itemize}
	\item uncompressed data sets rated with relative measures, such as the $mser_{rel}$, equ. (\ref{equ:mserrel}), for severe compression, i.e. $cr_t > 50$ and
	\item pre-compressed data sets\footnote{If absolutely necessary because of ground-truth availability, of course uncompressed data is preferred} with absolute measures, such as $mser_{abs}$, equ. (\ref{equ:mserabs}), for medium and light compression, i.e. $cr_t \leq 50$.
\end{itemize}
Furthermore, from observations \ref{exponentialGrowth} and \ref{bestMethods} we suggest to compress the iris images with TODO


%TODO TB:
%\begin{itemize}
% \item Transform CR's in bpp, compare to \cite{severeCompression}
%\end{itemize}



\section{Results}
\label{section:results}
Besides evaluating the segmentation error rate and the general purpose quality measures independently, their correlation is analyzed. Furthermore, we explicitly investigate the correlation between $mser_{rel}$ and $mser_{abs}$ to back up the observations in section \ref{section:ser}.
Since our standard choice, the Spearman’s rank correlation coefficient, did not well represent the subjective opinions of the paper’s authors for different graphs of the dataset, we used Dynamic Time Warping (DTW) as a second correlation metric.


Although the spearman’s rank correlation coefficient is a standard and powerful tool in order to determine the correlation between two graphs, this coefficient especially points out the overall trend of two graphs by using a monotonic function. Figure \ref{fig:corrSRCCproblem} shows one specific example of our dataset where the SRCC results in a high linear relationship since both graphs are close to monotonic functions. However, clearly both graphs are showing a significant difference in terms of a point-wise comparison (e.g. the Euclidean distance).

% Make a bit smaller, grid lines, B/W graph. Probably there are better examples, I'd give one hear that correlates really good, althoug spearman is bad and an opposite example. I suggest a few from our test data later.
\begin{figure}[h]
	\begin{center}
		
	\includegraphics[width=1\linewidth]{img/corrSRCCproblem}
\end{center}
	\caption{PSNR vs. EER (CAHT) for jpeg single compression: SRCC = 0.88 and ED = 0.53 . Note that both graphs were normalized to the interval [0,1] and PSNR additionally inverted.}
	\label{fig:corrSRCCproblem}
	
\end{figure}

\subsection{Correlation of Evaluation methods}
TODO: Martin:
Provide sensible correlation results and analyse

\subsubsection{Quality metrics vs. segmentation error}
TODO: Martin

\subsubsection{Segmentation error: relative vs. absolute}
TODO: Martin


\section{Conclusion}
\label{section:conclusion}
TODO: Martin 

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
