\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{icb}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\DeclareMathOperator*{\argmin}{arg\,min}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%\icbfinalcopy % *** Uncomment this line for the final submission

\def\icbPaperID{****} % *** Enter the IJCB Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificbfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Effects of using pre-compressed data for compression studies in iris recognition}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}






\maketitle
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous ICB abstracts to get a feel for style and length. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Done by Mr. Uhl, including related work?

\section{Test data generation and compression scheme}
\label{section:comprScheme}
We investigate whether using already pre-compressed data as a basis for image compression performance rating in iris biometric systems produces a different outcame than if truly uncompressed data has been used for the same rating process. Using pre-compressed data, this means when compression is applied, a pre-compressed image $I_p$ is compressed a second time, hence we denote this process as \emph{double-compression} resulting in an image $I_d$. When compressing a truly uncompressed image $I_u$, which was read directly from the sensor, we denote this as \emph{single-compression} with a resulting image $I_s$. Since experiments are typically carried out on a data set with more than one image, we denote $I_u^{(k)}, I_p^{(k)}, I_s^{(k)}, I_d^{(k)} \in \mathbb{R}^{w \times h}$ as the $k^{th}$ image with width $w$ and height $h$ in the particular data sets. Furthermore, we denote $s(F) \in \mathbb{N}$ with $F \sim{I \in \mathbb{R}^{w \times h}}$ as a function that returns the file size of the file $F$ storing an image $I \in \mathbb{R}^{w \times h} $. Since common lossy compression algorithms also employ lossless compression methods on the pixel data before writing to a file, $F$ is loosely linked to the contained pixel data $I$. For simplicity, we denote $s(I)$ as the file size of the file $F$ encoding the pixel values of an image $I$. $c_{m}(I, q)$ with $q \in \mathbb{N}$ defines the compression of an image $I$ with a particular method $m$ using the quality parameter $p$. In terms of this paper we use the values $m \in \{jpg, jxr, j2k\}$, where 
\begin{itemize}
	\item $jpg$ corresponds to the well-known (ISO/IEC IS 10918-1) DCT-based image compression method \cite{jpg},
	\item $j2k$ corresponds to the wavelet-based image compression standard (ISO/IEC IS 15444-1), which can operate at higher compression ratios \cite{j2k} and
	\item $jxr$ corresponds to a compression standard based on Microsoft’s HD Photo, which is specified in (ISO/IEC IS 29199-2) \cite{jxr}.  
\end{itemize}

%TODO: Consider: Do I have to specify which methods we used (Matlab JxrEncApp) to come up with the images? As it is a standard, this shouldn't be necessary, because everything operates the same anyway?!

For rating an image §I§'s compression effectiveness, we define the compression ratio $cr$ between an uncompressed image $I_u$ and a compressed image $I_c$ as 
\begin{equation}
cr(I_u, I_c) = \frac{s(I_u)}{s(I_c)} \quad \text{with} \quad I_c \in \{I_d, I_s\}
\end{equation}

For the later described experiments, images are compressed to a target compression ratio $cr_t \in \mathbb{R}$. All three compression methods take one parameter $q \in \mathbb{N}$ only, which controls the image quality but not the file size of the output image. Hence it is not possible to set this parameter in a way to meet a certain target compression ratio $cr_t$. Due to the limited universe of the quality parameters, the target compression ratio $cr_t$ cannot be achieved exactly, but a parameter optimization can be done, so that $cr_t \approxeq cr(I_u^{(k)}, I_c^{(k)})$. We propose an algorithm to compress a data set of $K$ uncompressed images $I_u$ using a particular method $m$ to achieve a certain compression ratio $cr_t := \gamma$ with $\gamma \in \mathbb{R}$ , e.g. $cr_t := 50$ in a way that the compression ratio of each image is met as close as possible. This process, illustrated in Fig. \ref{fig:comprScheme}, is defined as

\begin{figure}[h]
	\begin{center}
		
	\includegraphics[width=0.7\linewidth]{img/comprScheme}
\end{center}
	\caption{Basic compression principle to obtain two images achieving approximately the same target compression ratio $cr_t$ from an uncompressed image $I_u^{(k)}$ using a particular compression method $m$. One image, $I_s^{(k)}$, is compressed in a single step while the other, $I_d^{(k)}$, uses a pre-compression and a final compression step. Note the pre-compression step is always a $jpg$-compression, while the final compression step uses the same method $m$ as used in the single-compression.}
	\label{fig:comprScheme}
	
\end{figure}

\begin{enumerate}
	\item Compute the single-compressed image $I_s^{(k)}$ with method $m$ such that $cr(I_u^{(k)}, I_s^{(k)}) \approx cr_t$. The optimal quality parameter $q_s^{(k)}$ is computed for each image separately by solving
	\begin{eqnarray}
	\small
	s_t^{(k)} = \frac{s(I_u^{(k)})}{cr_t} \\
		q_s^{(k)} = \underset{q \in \mathbb{N}}{argmin}|s(c_m(I_u^{(k)},q)) - s_t^{(k)}|,
	\end{eqnarray} where $s_t^{(k)}$ is the file size to meet the target compression ratio $cr_t$. This is implemented by iteratively searching for the quality parameter $q$ that results in the compression ratio $cr(I_u, I_c)$ closest to $cr_t$. 
	\item The single compressed images $I_s^{(k)}$ with method $m$ are then computed with the found optimal parameters as
	\begin{equation}
	I_s^{(k)} = c_m(I_u^{(k)}, q_s^{(k)})
	\end{equation}
	
	\item Compute a pre-compressed image $I_p^{(k)}$ representing images in pre-compressed data set. This is done using $jpg$-method \cite{jpg} with an arbitrary but fixed quality parameter $q_p$. Note that pre-compressed images of one data set may have significant varying file sizes depending on the image's content.
	
	\item Now, find a quality parameter $q_d^{(k)}$ that allows to compress the pre-compressed image $I_p^{(k)}$ a second time, such that the resulting double-compressed image $I_d^{(k)}$ has the same file size as the single-compressed image $I_s^{(k)}$, i.e. $s(I_s^{(k)}) \approxeq s(I_d^{(k)})$. Such a quality parameter can be found by optimizing
	\begin{equation}
	\small
		q_d^{(k)} = \underset{q \in \mathbb{N}}{argmin}|s(c_m(I_p^{(k)},q)) - s(I_s^{(k)})| \quad \forall s(I_s^{(k)}) \geq s(I_d^{(k)})
	\end{equation}
	
	The condition $s(I_s^{(k)}) \geq s(I_d^{(k)})$ is of importance to establish fair conditions, since it is very likely that the size cannot be equalized due to the limited universe of the quality parameters of the used compression methods $m$.
	
	\item The double-compressed images $I_d^{(k)}$ are then computed from the pre-compressed images $I_p^{(k)}$ with the found optimal parameters as
	\begin{equation}
		I_d^{(k)} = c_m(I_p^{(k)}, q_d^{(k)})
	\end{equation}
	
\end{enumerate}

\section{Experimental setup}
Although there are several iris data sets around, few are available in uncompressed format. We use the IITD Iris data base\footnote{IITD Iris Database version 1.0, http://www4.comp.polyu.edu.hk/\textasciitilde csajaykr/IITD/Database\_Iris.htm}. The main reason for this is the availability of a segmentation ground truth created by an expert, which was recently introduced by Rathgeb \etal in \cite{severeCompression}. The $k^{th}$ image of this segmentation ground truth data set is subsequently denoted as $SGT^{(k)}$. They also propose segmentation error results compareable to those we define in equation (\ref{equ:mserabs}).
We want to point out that although it is claimed that the data set is in uncompressed BMP format (backed by \cite{severeCompression}), visual evaluation suggests there are block artefacts contained, potentially employed by compression. Although this is the case, using this data set was necessary due to the available ground truth, for reasons discussed in section \ref{section:ser}   %TODO Can we write it this way? Does this void the complete results?!

We use the scheme introduced in section \ref{section:comprScheme} and compress the data for the following set of target compression ratios:
\begin{equation}
cr_t \in \{5,10,15,20,25,30,35,40,50,60,75\}
\end{equation}
For each of these target compression ratios $cr_t$, the pre-compression step in double-compression mode is carried out with the quality parameters
\begin{equation}
q_p \in \{100, 85, 75, 70\}
\end{equation} 
in order to simulate different levels of pre-compression. Each of these combinations is used to compress with the introduced $jpg$, $j2k$ and $jxr$ methods. This results in a total of 165 data sets with 2240 images each.

\textbf{TODO: Statistics of compression accuracy (mean) and stddev}.

This test data set is used to test six implementations of iris recognition algorithms available in the University of Salzburg Iris Toolbox USIT \cite{rathgeb}. 

%TODO should we discuss used methods, i.e. GIMP for converting from JXR to PNG?


\section{Evaluation}
 Besides assessing the image quality with fully-referenced metrics, we investigate the behaviour of segmentation error rate and the system's EER in respect to the compression ratio.


\subsection{Full-referenced quality metrics}
Todo Lefteris:
\begin{itemize}
 \item Which quality metrics were in the selection
 \item Which were chosen
 \item Why have you chosen these
 \item Give a very very brief introduction (rather referencing!) of what the quality metrics are about and the most characteristic features
 \item Results and findings of this evaluation
\end{itemize}

\subsection{Segmentation error rates}
\label{section:ser}
The segmentation of an iris image is by nature a critical part in the process. We investigate the differences of single- and double compression as well as the aspects of using an absolute reference, e.g. a ground truth, or a relative one, e.g. the segmentation of the uncompressed images $I_u$.

\begin{figure}
\begin{center}

  \includegraphics[width=0.3\linewidth]{img/segMasks/gt.png}
  \includegraphics[width=0.3\linewidth]{img/segMasks/jpg_caht_q100_cr5.png}
  \includegraphics[width=0.3\linewidth]{img/segMasks/jpg_wahet_q100_cr5.png}
  \end{center}
  
  \label{fig:segMasks}
  \caption{Segmentation masks of the expert ground truth \cite{severeCompression}, relative groundtruth $seg(I_u^{(k)})$ and an actual segmentation result $seg(I_d^{(k)})$ (f.l.t.r)}
\end{figure}

%TODO discuss, why the eye lashes are notmasked in the algos


The segmentation accuracy is rated by the mean segmentation error rate, which corresponds to the suggested E1 error rate in the Noisy Iris Challenge Evaluation - Part I (NICE.I), also used in \cite{severeCompression}. We define the segmentation error rate $ser$ of a single image $I$ as
\begin{equation}
ser(R,S) = \overline{R \oplus S} \in [0,1]\quad with \quad R,S \in \{0,1\}^{w \times h},
\end{equation} where $R$ is the binarized reference segmentation and $S$ the binarized segmentation result of the same image $I$. The mean value of the pixel-wise exclusive-or is the percentage of pixels different in the segmented image $S$ in respect to the reference $R$. Due to multiple images in a data base, the mean segmentation error $mser$ is computed from $K$ images. In this experiment, we compute error rates for the single- and double compressed images $I_c \in {I_s, I_d}$. We define the absolute mean segmentation error $mser_{abs}$ in respect to the ground truth $SGT$ from \cite{severeCompression} and the relative mean segmentation error $mser_{rel}$ in respect to the segmentation of the uncompressed images $I_u$. By denoting the segmentation process of an image $I$ as $seg(I) \in \{0,1\}^{w \times h}$ we have
\begin{eqnarray}
mser_{abs} = \frac{1}{K}\sum_{k=1}^{K}ser(SGT^{(k)},seg(I_c^{(k)})) \label{equ:mserabs} \\
mser_{rel} = \frac{1}{K}\sum_{k=1}^{K}ser(seg(I_u^{(k)}),seg(I_c^{(k)})) \label{equ:mserrel} \\
\end{eqnarray}

The absolute segmentation error rate, hence also the development in respect to compression ratio can be considered to be optimal because of the available ground truth. However, for most data bases no such ground truth is available. therefore we evaluate if a corresponding conclusion can be withdrawn from the relative segmentation error $mser_{rel}$ as well. The benefit of such a relation is that the $mser_{rel}$ can be computed for any arbitrary data set easily. 

\begin{figure}[h]
	\begin{center}
		
	\includegraphics[width=\linewidth]{img/segResults}
\end{center}
	\caption{Top: Relative segmentation results with CAHT on $jpg$-compressed (a) and WAHET on $jxr$-compressed data (b). Bottom: Absolute segmentation results (c) and (d) with the same methods as in (a) and (b).}
	\label{fig:segResults}
	
\end{figure}

The experiments were carried out for CAHT and WAHET segmentation \cite{rathgeb}. From the graphs in Figure \ref{fig:segResults} we observe that

\begin{itemize}
 \item there is no significant difference in segmenting single-compressed or double-compressed images for medium compression ratios, i.e. $ 20 \leq cr_t \leq 50 $,
 \item there is a difference for small compression ratios, i.e. $cr_t < 20$, and large compression ratios, i.e. $cr_t > 50$. However, there is no trend in segmentation accuracy observeable, when investigating the difference between the single- or the double-compressed images and% TODO rephrase
 \item the $mser_{abs}$ is generally higher than $mser_{rel}$, because the segmentation algorithms ignore eyelids, yet are considered in the expert ground truth \cite{severeCompression}.
 \item Comparing $mser_{rel}$ and $mser_{abs}$ indicates that in most cases both metrics show the same characteristics, especially for medium and large compression ratios, i.e. $cr_t > 20$. For the WAHET segmentation, however, the results obtained with $jxr$ and $j2k$ compressed data contradict this observation. Hence, in a general case a ground truth is indeed needed to investigate the impact of compression on iris segmentation algorithms. Since this was concluded from two segmentation methods only, further investigation is necessary for this aspect.
\end{itemize}




TODO TB:
\begin{itemize}
 \item Transform CR's in bpp, compare to \cite{severeCompression}
\end{itemize}


\subsection{Equal Error rate}
To assess the total impact on the System, the EER is computed
TODO TB

\begin{itemize}
 \item Brief introduction
 \item Results and findings of this evaluation
\end{itemize}


\section{Results}
\subsection{Schnoell-Correlation-method}
TODO: Martin: Introduce your method here and argue why it is better than spearman

\subsection{Correlation of Evaluation methods}
TODO: Martin:
Provide sensible correlation results and analyse


\section{Conclusion}
TODO: Martin Schnöll


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
